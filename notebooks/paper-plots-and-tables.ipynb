{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a436d249-9dfc-4d25-9695-549cb440ea18",
   "metadata": {},
   "source": [
    "# Render paper plots and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7df2c18-8ad5-4733-b77e-68afde7064a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247e64e-8d4c-4dc7-beb2-cf70e1492fc2",
   "metadata": {},
   "source": [
    "Load aggregated results data\n",
    "(can be obtained using the `parse-acs-results.ipynb` notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4d0fde-5cea-4ed8-b496-00627aac840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACS_AGG_RESULTS_PATH = Path(\"/fast/groups/sf/folktexts-results\") / \"2024-07-03\" / \"aggregated_results.2024.07.09-23.12.24.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc248c4-a5c5-4c95-9f2a-3d386c830520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_diff</th>\n",
       "      <th>accuracy_ratio</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>balanced_accuracy_diff</th>\n",
       "      <th>balanced_accuracy_ratio</th>\n",
       "      <th>brier_score_loss</th>\n",
       "      <th>ece</th>\n",
       "      <th>ece_quantile</th>\n",
       "      <th>equalized_odds_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>name</th>\n",
       "      <th>is_inst</th>\n",
       "      <th>num_features</th>\n",
       "      <th>uses_all_features</th>\n",
       "      <th>fit_thresh_on_100</th>\n",
       "      <th>fit_thresh_accuracy</th>\n",
       "      <th>optimal_thresh</th>\n",
       "      <th>optimal_thresh_accuracy</th>\n",
       "      <th>score_stdev</th>\n",
       "      <th>score_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemma-2-9b-it__ACSIncome__-1</th>\n",
       "      <td>0.780030</td>\n",
       "      <td>0.096459</td>\n",
       "      <td>0.888661</td>\n",
       "      <td>0.753813</td>\n",
       "      <td>0.249373</td>\n",
       "      <td>0.688385</td>\n",
       "      <td>0.203177</td>\n",
       "      <td>0.194715</td>\n",
       "      <td>0.179097</td>\n",
       "      <td>0.694508</td>\n",
       "      <td>...</td>\n",
       "      <td>Gemma 2 9B (it)</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.085142</td>\n",
       "      <td>0.777837</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.761892</td>\n",
       "      <td>0.447796</td>\n",
       "      <td>0.328140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3-70B__ACSTravelTime__-1</th>\n",
       "      <td>0.548924</td>\n",
       "      <td>0.231092</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.591093</td>\n",
       "      <td>0.119547</td>\n",
       "      <td>0.804942</td>\n",
       "      <td>0.241007</td>\n",
       "      <td>0.092395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317821</td>\n",
       "      <td>...</td>\n",
       "      <td>Llama 3 70B</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.531220</td>\n",
       "      <td>0.614482</td>\n",
       "      <td>0.531059</td>\n",
       "      <td>0.608148</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.521286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     accuracy  accuracy_diff  accuracy_ratio  \\\n",
       "gemma-2-9b-it__ACSIncome__-1         0.780030       0.096459        0.888661   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1  0.548924       0.231092        0.607143   \n",
       "\n",
       "                                     balanced_accuracy  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                  0.753813   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1           0.591093   \n",
       "\n",
       "                                     balanced_accuracy_diff  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                       0.249373   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1                0.119547   \n",
       "\n",
       "                                     balanced_accuracy_ratio  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                        0.688385   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1                 0.804942   \n",
       "\n",
       "                                     brier_score_loss       ece  ece_quantile  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                 0.203177  0.194715      0.179097   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1          0.241007  0.092395           NaN   \n",
       "\n",
       "                                     equalized_odds_diff  ...  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                    0.694508  ...   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1             0.317821  ...   \n",
       "\n",
       "                                                name  is_inst  num_features  \\\n",
       "gemma-2-9b-it__ACSIncome__-1         Gemma 2 9B (it)     True            -1   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1      Llama 3 70B    False            -1   \n",
       "\n",
       "                                     uses_all_features  fit_thresh_on_100  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                      True           0.085142   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1               True           0.531220   \n",
       "\n",
       "                                     fit_thresh_accuracy  optimal_thresh  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                    0.777837        0.007575   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1             0.614482        0.531059   \n",
       "\n",
       "                                     optimal_thresh_accuracy score_stdev  \\\n",
       "gemma-2-9b-it__ACSIncome__-1                        0.761892    0.447796   \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1                 0.608148    0.048256   \n",
       "\n",
       "                                     score_mean  \n",
       "gemma-2-9b-it__ACSIncome__-1           0.328140  \n",
       "Meta-Llama-3-70B__ACSTravelTime__-1    0.521286  \n",
       "\n",
       "[2 rows x 64 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.read_csv(ACS_AGG_RESULTS_PATH, index_col=0)\n",
    "results_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658cc4b-e0b0-4dd0-a4c6-00c5ed79d4bf",
   "metadata": {},
   "source": [
    "### Run baseline ML classifiers on the benchmark ACS tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f02d293-7725-4754-bce8-caa40c93a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/fast/groups/sf\") / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5299d2a9-3723-44cc-a88b-fbba1d631eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_TASKS = [\n",
    "    \"ACSIncome\",\n",
    "    \"ACSMobility\",\n",
    "    \"ACSEmployment\",\n",
    "    \"ACSTravelTime\",\n",
    "    \"ACSPublicCoverage\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b72e20-a5d6-442b-8ba9-f7402f87b77b",
   "metadata": {},
   "source": [
    "List all baseline classifiers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1749fee2-ea21-41e9-b7f0-9ae5d0868409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier    # NOTE: requires `pip install xgboost`\n",
    "\n",
    "baselines = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"GBM\": HistGradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fb9307-a5c1-4805-8656-cc352cc736d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.acs.acs_dataset import ACSDataset\n",
    "from folktexts.evaluation import evaluate_predictions\n",
    "from collections import defaultdict\n",
    "\n",
    "def fit_and_eval(\n",
    "    clf,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test, s_test,\n",
    "    fillna=False,\n",
    ") -> dict:\n",
    "    \"\"\"Fit and evaluate a given classifier on the given data.\"\"\"\n",
    "    assert len(X_train) == len(y_train) and len(X_test) == len(y_test) == len(s_test)\n",
    "\n",
    "    train_nan_count = X_train.isna().any(axis=1).sum()\n",
    "    if fillna and train_nan_count > 0:\n",
    "        # Fill NaNs with value=-1\n",
    "        X_train = X_train.fillna(axis=\"columns\", value=-1)\n",
    "        X_test = X_test.fillna(axis=\"columns\", value=-1)\n",
    "\n",
    "    # Fit on train data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_test_scores = clf.predict_proba(X_test)[:, -1]\n",
    "    return evaluate_predictions(\n",
    "        y_true=y_test.to_numpy(),\n",
    "        y_pred_scores=y_test_scores,\n",
    "        sensitive_attribute=s_test,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "\n",
    "def run_baselines(baselines, tasks) -> dict:\n",
    "    \"\"\"Run baseline classifiers on all acs tasks.\"\"\"\n",
    "    baseline_results = defaultdict(dict)\n",
    "\n",
    "    # Prepare progress bar\n",
    "    progress_bar = tqdm(\n",
    "        total=len(tasks) * len(baselines),\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    for task in tasks:\n",
    "        progress_bar.set_postfix({\"task\": task})\n",
    "\n",
    "        # Load ACS task data\n",
    "        acs_dataset = ACSDataset.make_from_task(task=task, cache_dir=DATA_DIR)\n",
    "    \n",
    "        # Get train/test data\n",
    "        X_train, y_train = acs_dataset.get_train()\n",
    "        X_test, y_test = acs_dataset.get_test()\n",
    "    \n",
    "        # Get sensitive attribute test data\n",
    "        s_test = None\n",
    "        if acs_dataset.task.sensitive_attribute is not None:\n",
    "            s_test = acs_dataset.get_sensitive_attribute_data().loc[y_test.index]\n",
    "    \n",
    "        for clf_name, clf in baselines.items():\n",
    "            progress_bar.set_postfix({\"task\": task, \"clf\": clf_name})\n",
    "\n",
    "            try:\n",
    "                baseline_results[task][clf_name] = fit_and_eval(\n",
    "                    clf=clf,\n",
    "                    X_train=X_train, y_train=y_train,\n",
    "                    X_test=X_test, y_test=y_test, s_test=s_test,\n",
    "                    fillna=(clf_name == \"LR\"),\n",
    "                )\n",
    "            except Exception as err:\n",
    "                logging.error(err)\n",
    "            finally:\n",
    "                progress_bar.update()\n",
    "\n",
    "    return baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359a7a3-af13-4516-a07f-7d09f71f99ac",
   "metadata": {},
   "source": [
    "Flatten results and add extra columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4580a3a8-279d-4286-a8d0-01d228b0d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_baseline_results(baseline_results) -> list:\n",
    "    \"\"\"Flatten and parse baseline results.\"\"\"\n",
    "    parsed_results_list = list()\n",
    "    \n",
    "    for task, task_results in baseline_results.items():\n",
    "    \n",
    "        for clf, clf_results in task_results.items():\n",
    "            parsed_results = clf_results.copy()\n",
    "    \n",
    "            parsed_results[\"config_task_name\"] = task\n",
    "            parsed_results[\"config_model_name\"] = clf\n",
    "            parsed_results[\"name\"] = clf\n",
    "            parsed_results[\"num_features\"] = -1\n",
    "            parsed_results[\"uses_all_features\"] = True\n",
    "    \n",
    "            parsed_results_list.append(parsed_results)\n",
    "\n",
    "    return parsed_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0dd85-79f0-4a6b-a2e1-72fbeb84d85a",
   "metadata": {},
   "source": [
    "Check if baseline results were already computed. If so, load csv; otherwise, compute and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04b361c-780c-4e04-a817-6b2409db84e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b697c2b419554bcf9eba36f493ee4596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acruz/miniconda3/envs/folktexts/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acruz/miniconda3/envs/folktexts/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acruz/miniconda3/envs/folktexts/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acruz/miniconda3/envs/folktexts/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acruz/miniconda3/envs/folktexts/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_positives</th>\n",
       "      <th>n_negatives</th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fnr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>tnr</th>\n",
       "      <th>...</th>\n",
       "      <th>equalized_odds_ratio</th>\n",
       "      <th>equalized_odds_diff</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>ece</th>\n",
       "      <th>ece_quantile</th>\n",
       "      <th>config_task_name</th>\n",
       "      <th>config_model_name</th>\n",
       "      <th>name</th>\n",
       "      <th>num_features</th>\n",
       "      <th>uses_all_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.5</td>\n",
       "      <td>166450</td>\n",
       "      <td>61233</td>\n",
       "      <td>105217</td>\n",
       "      <td>None</td>\n",
       "      <td>0.817309</td>\n",
       "      <td>0.732089</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>0.133096</td>\n",
       "      <td>0.866904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077643</td>\n",
       "      <td>0.519188</td>\n",
       "      <td>0.894961</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.00362</td>\n",
       "      <td>ACSIncome</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.5</td>\n",
       "      <td>146665</td>\n",
       "      <td>64285</td>\n",
       "      <td>82380</td>\n",
       "      <td>None</td>\n",
       "      <td>0.702997</td>\n",
       "      <td>0.547453</td>\n",
       "      <td>0.452547</td>\n",
       "      <td>0.175625</td>\n",
       "      <td>0.824375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067760</td>\n",
       "      <td>0.306513</td>\n",
       "      <td>0.769540</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.01798</td>\n",
       "      <td>ACSTravelTime</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         threshold  n_samples  n_positives  n_negatives model_name  accuracy  \\\n",
       "XGBoost        0.5     166450        61233       105217       None  0.817309   \n",
       "XGBoost        0.5     146665        64285        82380       None  0.702997   \n",
       "\n",
       "              tpr       fnr       fpr       tnr  ...  equalized_odds_ratio  \\\n",
       "XGBoost  0.732089  0.267911  0.133096  0.866904  ...              0.077643   \n",
       "XGBoost  0.547453  0.452547  0.175625  0.824375  ...              0.067760   \n",
       "\n",
       "         equalized_odds_diff   roc_auc       ece  ece_quantile  \\\n",
       "XGBoost             0.519188  0.894961  0.004253       0.00362   \n",
       "XGBoost             0.306513  0.769540  0.017382       0.01798   \n",
       "\n",
       "         config_task_name  config_model_name     name  num_features  \\\n",
       "XGBoost         ACSIncome            XGBoost  XGBoost            -1   \n",
       "XGBoost     ACSTravelTime            XGBoost  XGBoost            -1   \n",
       "\n",
       "         uses_all_features  \n",
       "XGBoost               True  \n",
       "XGBoost               True  \n",
       "\n",
       "[2 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASELINE_RESULTS_PATH = ACS_AGG_RESULTS_PATH.parent / \"baseline-results.csv\"\n",
    "\n",
    "# If saved results exists: load\n",
    "if BASELINE_RESULTS_PATH.exists():\n",
    "    print(f\"Loading pre-computed baseline results from {BASELINE_RESULTS_PATH.as_posix()}\")\n",
    "    baselines_df = pd.read_csv(BASELINE_RESULTS_PATH, index_col=0)\n",
    "\n",
    "# Compute baseline results\n",
    "else:\n",
    "    # Compute baseline results\n",
    "    baseline_results = run_baselines(baselines, tasks=ALL_TASKS)\n",
    "\n",
    "    # Parse results\n",
    "    parsed_results_list = parse_baseline_results(baseline_results)\n",
    "\n",
    "    # Construct DF\n",
    "    baselines_df = pd.DataFrame(parsed_results_list, index=[r[\"name\"] for r in parsed_results_list])\n",
    "\n",
    "    # Save DF to disk\n",
    "    baselines_df.to_csv(BASELINE_RESULTS_PATH)\n",
    "\n",
    "# Show 2 random rows\n",
    "baselines_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b4ddea-bd88-4bee-87cc-b6aaa6af5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_results_df.shape=(115, 64)\n"
     ]
    }
   ],
   "source": [
    "all_results_df = pd.concat((results_df, baselines_df))\n",
    "print(f\"{all_results_df.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b657133-120f-4ddb-95ef-b7106b9511fb",
   "metadata": {},
   "source": [
    "## Render results table for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317e5744-31e3-48ef-b051-484f79b01a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_metrics = [\"ece\", \"brier_score_loss\", \"roc_auc\", \"accuracy\", \"fit_thresh_accuracy\", \"score_stdev\"] #, \"score_mean\"]\n",
    "\n",
    "model_col = \"config_model_name\"\n",
    "task_col = \"config_task_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df28300-70ee-47d7-b7ad-23d06ed415cc",
   "metadata": {},
   "source": [
    "Add model size and model family columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41009307-905b-4498-b98e-972b589f68ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config_task_name   model_family\n",
       "ACSEmployment      -               3\n",
       "                   Gemma           8\n",
       "                   Llama           4\n",
       "                   Mistral         6\n",
       "                   Yi              2\n",
       "ACSIncome          -               3\n",
       "                   Gemma           8\n",
       "                   Llama           4\n",
       "                   Mistral         6\n",
       "                   Yi              2\n",
       "ACSMobility        -               3\n",
       "                   Gemma           8\n",
       "                   Llama           4\n",
       "                   Mistral         6\n",
       "                   Yi              2\n",
       "ACSPublicCoverage  -               3\n",
       "                   Gemma           8\n",
       "                   Llama           4\n",
       "                   Mistral         6\n",
       "                   Yi              2\n",
       "ACSTravelTime      -               3\n",
       "                   Gemma           8\n",
       "                   Llama           4\n",
       "                   Mistral         6\n",
       "                   Yi              2\n",
       "Name: accuracy, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from folktexts.llm_utils import get_model_size_B\n",
    "\n",
    "all_results_df[\"model_size\"] = [\n",
    "    (\n",
    "        get_model_size_B(row[\"name\"], default=float(\"nan\"))\n",
    "        if row[\"name\"] not in baselines else \"-\"\n",
    "    )\n",
    "    for _, row in all_results_df.iterrows()\n",
    "]\n",
    "\n",
    "def get_model_family(model_name) -> str:\n",
    "    if \"llama\" in model_name.lower():\n",
    "        return \"Llama\"\n",
    "    elif \"mistral\" in model_name.lower() or \"mixtral\" in model_name.lower():\n",
    "        return \"Mistral\"\n",
    "    elif \"gemma\" in model_name.lower():\n",
    "        return \"Gemma\"\n",
    "    elif \"yi\" in model_name.lower():\n",
    "        return \"Yi\"\n",
    "    elif \"qwen\" in model_name.lower():\n",
    "        return \"Qwen\"\n",
    "    else:\n",
    "        return \"-\"\n",
    "\n",
    "all_results_df[\"model_family\"] = [get_model_family(row[model_col]) for _, row in all_results_df.iterrows()]\n",
    "all_results_df.groupby([task_col, \"model_family\"])[\"accuracy\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "477e4f80-2c42-458b-a98c-b32a95c7d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** ACSINCOME ***\n",
      "\n",
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & ece & brier score loss & roc auc & accuracy & fit thresh accuracy & score stdev \\\\\n",
      "Model &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Yi 34B (chat) & 0.19 & 0.19 & 0.86 & 0.72 & 0.74 & 0.34 \\\\\n",
      "Yi 34B & 0.24 & 0.22 & 0.85 & 0.62 & 0.74 & 0.20 \\\\\n",
      "Mixtral 8x22B (it) & 0.21 & 0.22 & 0.85 & 0.76 & 0.75 & 0.48 \\\\\n",
      "Mixtral 8x22B & 0.17 & 0.19 & 0.85 & 0.68 & 0.77 & 0.10 \\\\\n",
      "Mixtral 8x7B (it) & 0.16 & 0.18 & 0.86 & 0.78 & 0.78 & 0.42 \\\\\n",
      "Mixtral 8x7B & 0.17 & 0.21 & 0.83 & 0.65 & 0.74 & 0.06 \\\\\n",
      "Mistral 7B (it) & 0.21 & 0.22 & 0.83 & 0.77 & 0.77 & 0.42 \\\\\n",
      "Mistral 7B & 0.20 & 0.23 & 0.80 & 0.73 & 0.74 & 0.04 \\\\\n",
      "Llama 3 70B (it) & 0.27 & 0.27 & 0.86 & 0.69 & 0.78 & 0.42 \\\\\n",
      "Llama 3 70B & 0.20 & 0.20 & 0.86 & 0.70 & 0.77 & 0.14 \\\\\n",
      "Llama 3 8B (it) & 0.32 & 0.30 & 0.85 & 0.62 & 0.78 & 0.37 \\\\\n",
      "Llama 3 8B & 0.25 & 0.26 & 0.81 & 0.38 & 0.69 & 0.05 \\\\\n",
      "Gemma 2 27B (it) & 0.34 & 0.35 & 0.71 & 0.57 & 0.67 & 0.36 \\\\\n",
      "Gemma 2 27B & 0.20 & 0.29 & 0.51 & 0.53 & 0.39 & 0.23 \\\\\n",
      "Gemma 2 9B (it) & 0.20 & 0.20 & 0.84 & 0.78 & 0.78 & 0.45 \\\\\n",
      "Gemma 2 9B & 0.26 & 0.25 & 0.83 & 0.40 & 0.72 & 0.06 \\\\\n",
      "Gemma 7B (it) & 0.61 & 0.59 & 0.84 & 0.37 & 0.75 & 0.06 \\\\\n",
      "Gemma 7B & 0.24 & 0.27 & 0.76 & 0.37 & 0.68 & 0.04 \\\\\n",
      "Gemma 2B (it) & 0.63 & 0.63 & 0.73 & 0.37 & 0.63 & 0.00 \\\\\n",
      "Gemma 2B & 0.14 & 0.25 & 0.62 & 0.45 & 0.63 & 0.02 \\\\\n",
      "LR & 0.03 & 0.18 & 0.79 & 0.74 & - & - \\\\\n",
      "GBM & 0.01 & 0.13 & 0.89 & 0.81 & - & - \\\\\n",
      "XGBoost & 0.00 & 0.13 & 0.90 & 0.82 & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "*** ACSMOBILITY ***\n",
      "\n",
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & ece & brier score loss & roc auc & accuracy & fit thresh accuracy & score stdev \\\\\n",
      "Model &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Yi 34B (chat) & 0.09 & 0.20 & 0.56 & 0.72 & 0.59 & 0.15 \\\\\n",
      "Yi 34B & 0.07 & 0.20 & 0.57 & 0.73 & 0.63 & 0.08 \\\\\n",
      "Mixtral 8x22B (it) & 0.41 & 0.40 & 0.51 & 0.40 & 0.50 & 0.24 \\\\\n",
      "Mixtral 8x22B & 0.11 & 0.20 & 0.55 & 0.73 & 0.63 & 0.02 \\\\\n",
      "Mixtral 8x7B (it) & 0.26 & 0.26 & 0.58 & 0.73 & 0.60 & 0.02 \\\\\n",
      "Mixtral 8x7B & 0.14 & 0.21 & 0.57 & 0.73 & 0.52 & 0.03 \\\\\n",
      "Mistral 7B (it) & 0.26 & 0.26 & 0.57 & 0.73 & 0.54 & 0.02 \\\\\n",
      "Mistral 7B & 0.20 & 0.23 & 0.53 & 0.73 & 0.33 & 0.01 \\\\\n",
      "Llama 3 70B (it) & 0.20 & 0.25 & 0.57 & 0.61 & 0.48 & 0.18 \\\\\n",
      "Llama 3 70B & 0.22 & 0.24 & 0.55 & 0.67 & 0.53 & 0.04 \\\\\n",
      "Llama 3 8B (it) & 0.15 & 0.22 & 0.56 & 0.70 & 0.53 & 0.17 \\\\\n",
      "Llama 3 8B & 0.10 & 0.20 & 0.55 & 0.73 & 0.63 & 0.04 \\\\\n",
      "Gemma 2 27B (it) & 0.38 & 0.39 & 0.51 & 0.46 & 0.37 & 0.33 \\\\\n",
      "Gemma 2 27B & 0.23 & 0.28 & 0.49 & 0.61 & 0.54 & 0.27 \\\\\n",
      "Gemma 2 9B (it) & 0.20 & 0.23 & 0.54 & 0.73 & 0.67 & 0.07 \\\\\n",
      "Gemma 2 9B & 0.31 & 0.29 & 0.54 & 0.29 & 0.42 & 0.04 \\\\\n",
      "Gemma 7B (it) & 0.25 & 0.26 & 0.58 & 0.73 & 0.67 & 0.11 \\\\\n",
      "Gemma 7B & 0.41 & 0.37 & 0.50 & 0.39 & 0.41 & 0.10 \\\\\n",
      "Gemma 2B (it) & 0.73 & 0.73 & 0.52 & 0.27 & 0.40 & 0.00 \\\\\n",
      "Gemma 2B & 0.25 & 0.26 & 0.51 & 0.39 & 0.45 & 0.02 \\\\\n",
      "LR & 0.02 & 0.19 & 0.61 & 0.74 & - & - \\\\\n",
      "GBM & 0.01 & 0.17 & 0.74 & 0.76 & - & - \\\\\n",
      "XGBoost & 0.00 & 0.16 & 0.74 & 0.76 & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "*** ACSEMPLOYMENT ***\n",
      "\n",
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & ece & brier score loss & roc auc & accuracy & fit thresh accuracy & score stdev \\\\\n",
      "Model &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Yi 34B (chat) & 0.14 & 0.21 & 0.79 & 0.69 & 0.73 & 0.20 \\\\\n",
      "Yi 34B & 0.08 & 0.23 & 0.70 & 0.62 & 0.63 & 0.11 \\\\\n",
      "Mixtral 8x22B (it) & 0.38 & 0.39 & 0.60 & 0.51 & 0.60 & 0.26 \\\\\n",
      "Mixtral 8x22B & 0.21 & 0.24 & 0.86 & 0.52 & 0.77 & 0.08 \\\\\n",
      "Mixtral 8x7B (it) & 0.22 & 0.23 & 0.83 & 0.73 & 0.75 & 0.44 \\\\\n",
      "Mixtral 8x7B & 0.29 & 0.31 & 0.81 & 0.45 & 0.74 & 0.06 \\\\\n",
      "Mistral 7B (it) & 0.35 & 0.36 & 0.72 & 0.63 & 0.66 & 0.49 \\\\\n",
      "Mistral 7B & 0.26 & 0.30 & 0.76 & 0.45 & 0.69 & 0.04 \\\\\n",
      "Llama 3 70B (it) & 0.17 & 0.19 & 0.85 & 0.73 & 0.78 & 0.29 \\\\\n",
      "Llama 3 70B & 0.25 & 0.26 & 0.82 & 0.51 & 0.74 & 0.12 \\\\\n",
      "Llama 3 8B (it) & 0.07 & 0.19 & 0.79 & 0.74 & 0.74 & 0.22 \\\\\n",
      "Llama 3 8B & 0.34 & 0.34 & 0.75 & 0.45 & 0.68 & 0.06 \\\\\n",
      "Gemma 2 27B (it) & 0.32 & 0.37 & 0.52 & 0.52 & 0.51 & 0.35 \\\\\n",
      "Gemma 2 27B & 0.26 & 0.34 & 0.50 & 0.49 & 0.52 & 0.27 \\\\\n",
      "Gemma 2 9B (it) & 0.16 & 0.20 & 0.82 & 0.70 & 0.74 & 0.25 \\\\\n",
      "Gemma 2 9B & 0.13 & 0.24 & 0.73 & 0.52 & 0.67 & 0.07 \\\\\n",
      "Gemma 7B (it) & 0.36 & 0.38 & 0.59 & 0.58 & 0.59 & 0.44 \\\\\n",
      "Gemma 7B & 0.16 & 0.25 & 0.65 & 0.48 & 0.64 & 0.13 \\\\\n",
      "Gemma 2B (it) & 0.38 & 0.41 & 0.42 & 0.46 & 0.48 & 0.23 \\\\\n",
      "Gemma 2B & 0.01 & 0.24 & 0.63 & 0.54 & 0.61 & 0.03 \\\\\n",
      "LR & 0.02 & 0.15 & 0.86 & 0.78 & - & - \\\\\n",
      "GBM & 0.00 & 0.12 & 0.91 & 0.83 & - & - \\\\\n",
      "XGBoost & 0.00 & 0.12 & 0.91 & 0.83 & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "*** ACSTRAVELTIME ***\n",
      "\n",
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & ece & brier score loss & roc auc & accuracy & fit thresh accuracy & score stdev \\\\\n",
      "Model &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Yi 34B (chat) & 0.35 & 0.36 & 0.65 & 0.56 & 0.52 & 0.02 \\\\\n",
      "Yi 34B & 0.08 & 0.24 & 0.62 & 0.56 & 0.52 & 0.06 \\\\\n",
      "Mixtral 8x22B (it) & 0.32 & 0.33 & 0.66 & 0.59 & 0.61 & 0.15 \\\\\n",
      "Mixtral 8x22B & 0.20 & 0.28 & 0.63 & 0.44 & 0.54 & 0.04 \\\\\n",
      "Mixtral 8x7B (it) & 0.45 & 0.45 & 0.66 & 0.52 & 0.55 & 0.29 \\\\\n",
      "Mixtral 8x7B & 0.28 & 0.32 & 0.60 & 0.44 & 0.56 & 0.03 \\\\\n",
      "Mistral 7B (it) & 0.41 & 0.42 & 0.59 & 0.57 & 0.53 & 0.19 \\\\\n",
      "Mistral 7B & 0.05 & 0.25 & 0.57 & 0.56 & 0.55 & 0.02 \\\\\n",
      "Llama 3 70B (it) & 0.15 & 0.24 & 0.70 & 0.60 & 0.61 & 0.12 \\\\\n",
      "Llama 3 70B & 0.09 & 0.24 & 0.67 & 0.55 & 0.61 & 0.05 \\\\\n",
      "Llama 3 8B (it) & 0.19 & 0.28 & 0.60 & 0.57 & 0.57 & 0.11 \\\\\n",
      "Llama 3 8B & 0.08 & 0.25 & 0.53 & 0.56 & 0.54 & 0.03 \\\\\n",
      "Gemma 2 27B (it) & 0.32 & 0.36 & 0.52 & 0.53 & 0.48 & 0.34 \\\\\n",
      "Gemma 2 27B & 0.23 & 0.32 & 0.50 & 0.49 & 0.46 & 0.25 \\\\\n",
      "Gemma 2 9B (it) & 0.07 & 0.23 & 0.67 & 0.60 & 0.57 & 0.15 \\\\\n",
      "Gemma 2 9B & 0.02 & 0.24 & 0.58 & 0.56 & 0.51 & 0.02 \\\\\n",
      "Gemma 7B (it) & 0.42 & 0.43 & 0.53 & 0.56 & 0.50 & 0.02 \\\\\n",
      "Gemma 7B & 0.04 & 0.24 & 0.61 & 0.58 & 0.57 & 0.02 \\\\\n",
      "Gemma 2B (it) & 0.34 & 0.36 & 0.49 & 0.56 & 0.54 & 0.03 \\\\\n",
      "Gemma 2B & 0.09 & 0.26 & 0.48 & 0.44 & 0.45 & 0.01 \\\\\n",
      "LR & 0.04 & 0.24 & 0.58 & 0.56 & - & - \\\\\n",
      "GBM & 0.02 & 0.20 & 0.75 & 0.69 & - & - \\\\\n",
      "XGBoost & 0.02 & 0.19 & 0.77 & 0.70 & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "*** ACSPUBLICCOVERAGE ***\n",
      "\n",
      "\\begin{tabular}{lrrrrll}\n",
      "\\toprule\n",
      " & ece & brier score loss & roc auc & accuracy & fit thresh accuracy & score stdev \\\\\n",
      "Model &  &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Yi 34B (chat) & 0.06 & 0.19 & 0.67 & 0.74 & 0.68 & 0.13 \\\\\n",
      "Yi 34B & 0.04 & 0.21 & 0.59 & 0.70 & 0.56 & 0.07 \\\\\n",
      "Mixtral 8x22B (it) & 0.24 & 0.25 & 0.70 & 0.72 & 0.67 & 0.17 \\\\\n",
      "Mixtral 8x22B & 0.32 & 0.30 & 0.59 & 0.30 & 0.56 & 0.05 \\\\\n",
      "Mixtral 8x7B (it) & 0.20 & 0.23 & 0.70 & 0.74 & 0.72 & 0.35 \\\\\n",
      "Mixtral 8x7B & 0.41 & 0.37 & 0.57 & 0.30 & 0.38 & 0.03 \\\\\n",
      "Mistral 7B (it) & 0.30 & 0.30 & 0.61 & 0.70 & 0.68 & 0.01 \\\\\n",
      "Mistral 7B & 0.29 & 0.30 & 0.45 & 0.30 & 0.70 & 0.02 \\\\\n",
      "Llama 3 70B (it) & 0.16 & 0.21 & 0.69 & 0.75 & 0.49 & 0.20 \\\\\n",
      "Llama 3 70B & 0.18 & 0.22 & 0.67 & 0.63 & 0.50 & 0.07 \\\\\n",
      "Llama 3 8B (it) & 0.11 & 0.21 & 0.59 & 0.71 & 0.71 & 0.19 \\\\\n",
      "Llama 3 8B & 0.41 & 0.38 & 0.55 & 0.30 & 0.50 & 0.02 \\\\\n",
      "Gemma 2 27B (it) & 0.24 & 0.29 & 0.54 & 0.63 & 0.57 & 0.30 \\\\\n",
      "Gemma 2 27B & 0.43 & 0.43 & 0.51 & 0.38 & 0.52 & 0.24 \\\\\n",
      "Gemma 2 9B (it) & 0.16 & 0.23 & 0.61 & 0.67 & 0.68 & 0.28 \\\\\n",
      "Gemma 2 9B & 0.20 & 0.25 & 0.50 & 0.38 & 0.65 & 0.02 \\\\\n",
      "Gemma 7B (it) & 0.30 & 0.34 & 0.46 & 0.50 & 0.32 & 0.32 \\\\\n",
      "Gemma 7B & 0.15 & 0.23 & 0.49 & 0.49 & 0.37 & 0.10 \\\\\n",
      "Gemma 2B (it) & 0.70 & 0.70 & 0.54 & 0.30 & 0.33 & 0.00 \\\\\n",
      "Gemma 2B & 0.26 & 0.28 & 0.54 & 0.30 & 0.62 & 0.01 \\\\\n",
      "LR & 0.03 & 0.19 & 0.70 & 0.73 & - & - \\\\\n",
      "GBM & 0.01 & 0.14 & 0.83 & 0.80 & - & - \\\\\n",
      "XGBoost & 0.00 & 0.14 & 0.84 & 0.80 & - & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import prettify_model_name\n",
    "\n",
    "for task in ALL_TASKS:\n",
    "    task_df = all_results_df[all_results_df[task_col] == task]\n",
    "\n",
    "    latex_table = task_df.sort_values([\"model_family\", \"model_size\", \"is_inst\"], ascending=False).set_index(model_col)[table_metrics].round(3)\n",
    "    latex_table = latex_table.rename(columns=lambda col: col.replace(\"_\", \" \")).fillna(\"-\")\n",
    "\n",
    "    # Prettify model names\n",
    "    latex_table[\"Model\"] = [\n",
    "        prettify_model_name(id_) if id_ not in baselines.keys() else id_\n",
    "        for id_, row in latex_table.iterrows()\n",
    "    ]\n",
    "    latex_table.set_index(\"Model\", drop=True, inplace=True)\n",
    "\n",
    "    print(f\"*** {task.upper()} ***\\n\")\n",
    "    print(latex_table.to_latex(float_format=\"%.2f\"))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4564ed-1e41-4322-8c87-74fc34265bf5",
   "metadata": {},
   "source": [
    "## Render plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8819b1-0033-4d44-ab1e-a4b55a4ebd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
