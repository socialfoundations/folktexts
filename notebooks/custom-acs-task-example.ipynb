{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run `folktexts` benchmark with a custom ACS task\n",
    "\n",
    "This notebook describes how to define a custom prediction task based on \n",
    "American Community Survey (ACS) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.18'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folktexts\n",
    "folktexts.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Change the `DATA_DIR` variable to the ACS data path on your file system (if any, otherwise will attempt to download data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/fast/groups/sf\") / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom task objects appropriately:\n",
    "\n",
    "In this example, we'll try to predict whether someone is or ever was part of the Military, based on a series of demographic features.\n",
    "\n",
    "We must define objects of the following classes:\n",
    "- `Threshold`: Used to binarize the target column (not needed if target is already binary);\n",
    "- `MultipleChoiceQA`: Defines a question and answer scheme for the target column;\n",
    "- `TaskMetadata`: Defines which columns to use as features, and which to use as the prediction target;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the target column (among those defined in `folktexts.acs.acs_columns`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = \"MIL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define threshold to be applied to target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.threshold import Threshold\n",
    "target_threshold = Threshold(4, \"!=\")\n",
    "\n",
    "# data[\"MIL\"] != 4 means \"Is on active duty in the military or was in the past\"\n",
    "# data[\"MIL\"] == 4 means \"Never served in the military\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define question and answer interface used for prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.qa_interface import MultipleChoiceQA\n",
    "from folktexts.qa_interface import Choice\n",
    "\n",
    "target_column_qa = MultipleChoiceQA(\n",
    "    column=target_threshold.apply_to_column_name(TARGET_COLUMN),\n",
    "    text=\"Has this person ever served in the military?\",\n",
    "    choices=(\n",
    "        Choice(\"Yes, this person is now on active duty or was in the past\", 1),\n",
    "        Choice(\"No, this person has never served in the military\", 0),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define task metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.acs import ACSTaskMetadata\n",
    "\n",
    "task = ACSTaskMetadata.make_task(\n",
    "    name=\"ACSMilitary\",\n",
    "    description=\"predict if a person has ever served in the military\",\n",
    "    features=[\n",
    "        \"AGEP\", \"SCHL\", \"MAR\", \"POBP\", \"WKHP\", \"SEX\", \"RAC1P\", \"ST\", \"CIT\", \"DIS\", \"PINCP\",\n",
    "    ],\n",
    "    target=TARGET_COLUMN,\n",
    "    target_threshold=target_threshold,\n",
    "    sensitive_attribute=\"RAC1P\",\n",
    "    qa_interface=target_column_qa,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ACS dataset for the custom task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ACS data...\n",
      "CPU times: user 47.3 s, sys: 19.4 s, total: 1min 6s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from folktexts.acs.acs_dataset import ACSDataset\n",
    "acs_dataset = ACSDataset.make_from_task(\n",
    "    task=task,\n",
    "    subsampling=0.1,           # NOTE: optionally, use subsampling for faster but noisier results\n",
    "    cache_dir=DATA_DIR,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This dataset will contain all samples in the ACS PUMS file, you can\n",
    "now filter the `acs_dataset.data` if you want to use only a portion of the data \n",
    "(e.g, only people in California, or only people above 18 years old).\n",
    "\n",
    "As an example, we'll filter the ACS data to contain only people above 18 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of samples: 3236107\n",
      "Parsed number of samples: 2580544\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original number of samples: {len(acs_dataset.data)}\")\n",
    "\n",
    "acs_dataset.data = acs_dataset.data[acs_dataset.data[\"AGEP\"] >= 18]\n",
    "print(f\"Parsed number of samples: {len(acs_dataset.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM from huggingface or from local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_OR_PATH = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0777146d5a44ff4bae28e6b597cc8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 s, sys: 7.41 s, total: 11.6 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from folktexts.llm_utils import load_model_tokenizer\n",
    "model, tokenizer = load_model_tokenizer(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `LLMClassifier` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.classifier import LLMClassifier\n",
    "\n",
    "llm_clf = LLMClassifier(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=task,\n",
    "    batch_size=20,\n",
    "    context_size=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation:** Check that model prompts with the custom task make sense, or if anything needs to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following data corresponds to a survey respondent. The survey was conducted among US residents in 2018. Please answer the question based on the information provided. The data provided is enough to reach an approximate answer.\n",
      "\n",
      "Information:\n",
      "- The age is: 26 years old.\n",
      "- The highest educational attainment is: 9th grade.\n",
      "- The marital status is: Never married.\n",
      "- The place of birth is: Georgia.\n",
      "- The usual number of hours worked per week is: 45 hours.\n",
      "- The sex is: Female.\n",
      "- The race is: White.\n",
      "- The resident state is: Georgia.\n",
      "- The citizenship status is: Born in the United States.\n",
      "- The disability status is: With a disability.\n",
      "- The yearly income is: $20,000.\n",
      "\n",
      "Question: Has this person ever served in the military?\n",
      "A. Yes, this person is now on active duty or was in the past.\n",
      "B. No, this person has never served in the military.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "X_sample, _y_sample = acs_dataset.sample_n_train_examples(n=1)\n",
    "print(llm_clf.encode_row(X_sample.iloc[0], question=llm_clf.task.question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run benchmark on the custom task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktexts.benchmark import Benchmark, BenchmarkConfig\n",
    "\n",
    "bench = Benchmark(\n",
    "    llm_clf=llm_clf,\n",
    "    dataset=acs_dataset,\n",
    "    config=BenchmarkConfig.default_config(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIL!=4\n",
       "0    1864068\n",
       "1     200367\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs_dataset.get_train()[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c4e6d64a0e4740a6a348f27b9ff707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing risk estimates:   0%|          | 0/12903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bench.run(results_root_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No results to plot. Run the benchmark first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m/lustre/home/acruz/folktexts/folktexts/benchmark.py:277\u001b[0m, in \u001b[0;36mBenchmark.plot_results\u001b[0;34m(self, show_plots)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Render evaluation plots and save to disk.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    The paths to the saved plots.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo results to plot. Run the benchmark first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m imgs_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m imgs_dir\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: No results to plot. Run the benchmark first."
     ]
    }
   ],
   "source": [
    "bench.plot_results();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
